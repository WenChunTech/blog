<!DOCTYPE html>
<html itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <head>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
    <meta name="robots" content="noodp" />
    <title>SelfAttention和MultiHeadAttention流程简述 - 技术小窝</title><meta name="author" content="技术小窝">
<meta name="author-link" content="">
<meta name="description" content="分享技术，记录技术成长过程" /><meta name="keywords" content='draft' /><meta itemprop="name" content="SelfAttention和MultiHeadAttention流程简述">
<meta itemprop="description" content=""><meta itemprop="datePublished" content="2023-05-07T11:38:09+08:00" />
<meta itemprop="dateModified" content="2023-05-07T11:38:09+08:00" />
<meta itemprop="wordCount" content="459"><meta itemprop="image" content="https://blog-12x.pages.dev/logo.png"/>
<meta itemprop="keywords" content="draft," /><meta property="og:title" content="SelfAttention和MultiHeadAttention流程简述" />
<meta property="og:description" content="" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://blog-12x.pages.dev/selfattention%E5%92%8Cmultiheadattention%E6%B5%81%E7%A8%8B%E7%AE%80%E8%BF%B0/" /><meta property="og:image" content="https://blog-12x.pages.dev/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-05-07T11:38:09+08:00" />
<meta property="article:modified_time" content="2023-05-07T11:38:09+08:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://blog-12x.pages.dev/logo.png"/>

<meta name="twitter:title" content="SelfAttention和MultiHeadAttention流程简述"/>
<meta name="twitter:description" content=""/>
<meta name="application-name" content="FixIt">
<meta name="apple-mobile-web-app-title" content="FixIt"><meta name="theme-color" data-light="#f8f8f8" data-dark="#252627" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://blog-12x.pages.dev/selfattention%E5%92%8Cmultiheadattention%E6%B5%81%E7%A8%8B%E7%AE%80%E8%BF%B0/" /><link rel="prev" href="https://blog-12x.pages.dev/%E4%BB%80%E4%B9%88%E6%98%AF%E4%BA%A4%E5%8F%89%E7%86%B5/" /><link rel="next" href="https://blog-12x.pages.dev/asyncio%E4%BD%BF%E7%94%A8/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "SelfAttention和MultiHeadAttention流程简述",
    "inLanguage": "zh-CN",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https:\/\/blog-12x.pages.dev\/selfattention%E5%92%8Cmultiheadattention%E6%B5%81%E7%A8%8B%E7%AE%80%E8%BF%B0\/"
    },"genre": "posts","keywords": "draft","wordcount":  459 ,
    "url": "https:\/\/blog-12x.pages.dev\/selfattention%E5%92%8Cmultiheadattention%E6%B5%81%E7%A8%8B%E7%AE%80%E8%BF%B0\/","datePublished": "2023-05-07T11:38:09+08:00","dateModified": "2023-05-07T11:38:09+08:00","publisher": {
      "@type": "Organization",
      "name": ""},"description": ""
  }
  </script></head>
  <body data-header-desktop="sticky" data-header-mobile="auto"><script>(window.localStorage?.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('data-theme', 'dark');</script><div class="wrapper"><header class="desktop animate__faster" id="header-desktop">
  <div class="header-wrapper">
    <div class="header-title">
      <a href="/" title="技术小窝"><span class="header-title-text">我的技术小窝</span></a><span class="header-subtitle"></span></div>
    <nav>
      <ul class="menu"><li class="menu-item">
              <a
                class="menu-link"
                href="/posts/"
                
                
              ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 文章</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/categories/"
                
                
              ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 分类</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/tags/"
                
                
              ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id="search-desktop">
            <input type="text" placeholder="搜索文章标题或内容……" id="search-input-desktop">
            <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
              <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
            </a>
            <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
              <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
            </a>
            <span class="search-button search-loading" id="search-loading-desktop">
              <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
            </span>
          </li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li>
      </ul>
    </nav>
  </div>
</header><header class="mobile animate__faster" id="header-mobile">
  <div class="header-container">
    <div class="header-wrapper">
      <div class="header-title">
        <a href="/" title="技术小窝"><span class="header-title-text">我的技术小窝</span></a><span class="header-subtitle"></span></div>
      <div class="menu-toggle" id="menu-toggle-mobile">
        <span></span><span></span><span></span>
      </div>
    </div>
    <nav>
      <ul class="menu" id="menu-mobile"><li class="search-wrapper">
            <div class="search mobile" id="search-mobile">
              <input type="text" placeholder="搜索文章标题或内容……" id="search-input-mobile">
              <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
              </a>
              <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
              </a>
              <span class="search-button search-loading" id="search-loading-mobile">
                <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
              </span>
            </div>
            <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
              取消
            </a>
          </li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/posts/"
                  
                  
                ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 文章</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/categories/"
                  
                  
                ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 分类</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/tags/"
                  
                  
                ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li></ul>
    </nav>
  </div>
</header><div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
  </div>
  <div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
  </div><main class="container" data-page-style="normal"><aside class="toc" id="toc-auto"><h2 class="toc-title">目录&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden="true"></i></h2>
      <div class="toc-content" id="toc-content-auto"></div></aside>

  <aside class="aside-custom">
    </aside>

  <article class="page single">
    <div class="header"><h1 class="single-title animate__animated animate__flipInX">
        <span title="转载" class="icon-repost"><i class="fa-solid fa-share fa-fw" aria-hidden="true"></i></span><span>SelfAttention和MultiHeadAttention流程简述</span>
      </h1></div><div class="post-meta">
      <div class="post-meta-line"><span class="post-author"><span class="author"><i class="fa-solid fa-user-circle" aria-hidden="true"></i>
      </span></span>
          <span class="post-category">收录于 <a href="/categories/draft/"><i class="fa-regular fa-folder fa-fw" aria-hidden="true"></i> draft</a></span></div>
      <div class="post-meta-line"><span title=2023-05-07&#32;11:38:09><i class="fa-regular fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2023-05-07">2023-05-07</time></span>&nbsp;<span><i class="fa-solid fa-pencil-alt fa-fw" aria-hidden="true"></i> 约 459 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw" aria-hidden="true"></i> 预计阅读 1 分钟</span>&nbsp;<span id="busuanzi_container_page_pv" class="busuanzi_visitors comment-visitors" data-flag-title="SelfAttention和MultiHeadAttention流程简述">
            <i class="fa-regular fa-eye fa-fw" aria-hidden="true"></i>&nbsp;<span id="busuanzi_value_page_pv">-</span>&nbsp;次阅读
          </span>&nbsp;</div>
    </div><div class="details toc" id="toc-static" data-kept="false">
        <div class="details-summary toc-title">
          <span>目录</span>
          <span><i class="details-icon fa-solid fa-angle-right" aria-hidden="true"></i></span>
        </div>
        <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#1-selfattention">1. SelfAttention</a></li>
  </ul>
</nav></div>
      </div><div class="content" id="content"><h1 id="selfattention和multiheadattention流程简述">SelfAttention和MultiHeadAttention流程简述</h1>
<h2 id="1-selfattention">1. SelfAttention</h2>
<ul>
<li><strong>1.1</strong> 对于每一个输入$I$,初始化3个权重矩阵$W_q,W_k,W_v$.</li>
<li><strong>1.2</strong> 输入$I$分别与3个权重矩阵相乘得到$Q,K,V$</li>
<li><strong>1.3</strong> 将$Q$与$K$相乘(矩阵乘法)得到$A$(attention score)</li>
<li><strong>1.4</strong> $A$经过softmax激活得到$A'$</li>
<li><strong>1.5</strong> 将$A&rsquo;$与$V$相乘得到B</li>
</ul>
<p><strong>代码示例</strong>:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span><span class="lnt">74
</span><span class="lnt">75
</span><span class="lnt">76
</span><span class="lnt">77
</span><span class="lnt">78
</span><span class="lnt">79
</span><span class="lnt">80
</span><span class="lnt">81
</span><span class="lnt">82
</span><span class="lnt">83
</span><span class="lnt">84
</span><span class="lnt">85
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">In [16]: i = torch.randn(3,4)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">In [17]: w_query = torch.randn(4, 2)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">In [18]: w_key = torch.randn(4, 2)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">In [19]: w_value = torch.randn(4, 2)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">In [20]: i
</span></span><span class="line"><span class="cl">Out[20]:
</span></span><span class="line"><span class="cl">tensor([[ 0.4313,  1.3749, -0.2489,  1.3275],
</span></span><span class="line"><span class="cl">        [-0.6467,  1.6335,  2.8923,  0.9124],
</span></span><span class="line"><span class="cl">        [ 0.2326, -0.2314,  0.3554,  0.0892]])
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">In [21]: w_query
</span></span><span class="line"><span class="cl">Out[21]:
</span></span><span class="line"><span class="cl">tensor([[ 0.3673, -0.8505],
</span></span><span class="line"><span class="cl">        [-0.3559, -0.3708],
</span></span><span class="line"><span class="cl">        [-1.2093,  0.6634],
</span></span><span class="line"><span class="cl">        [ 0.4042,  0.8015]])
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">In [22]: w_key
</span></span><span class="line"><span class="cl">Out[22]:
</span></span><span class="line"><span class="cl">tensor([[-1.1617,  0.4023],
</span></span><span class="line"><span class="cl">        [-0.1249, -0.1605],
</span></span><span class="line"><span class="cl">        [-0.8427,  1.1002],
</span></span><span class="line"><span class="cl">        [-1.1320,  0.4611]])
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">In [23]: w_value
</span></span><span class="line"><span class="cl">Out[23]:
</span></span><span class="line"><span class="cl">tensor([[-0.7675, -0.3179],
</span></span><span class="line"><span class="cl">        [-1.1910, -0.6937],
</span></span><span class="line"><span class="cl">        [-1.4809, -1.5502],
</span></span><span class="line"><span class="cl">        [-0.7373,  1.0511]])
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">In [24]: querys = i @ w_query
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">In [25]: keys = i @ w_key
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">In [26]: values = i @ w_value
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">In [27]: querys
</span></span><span class="line"><span class="cl">Out[27]:
</span></span><span class="line"><span class="cl">tensor([[ 0.5066,  0.0223],
</span></span><span class="line"><span class="cl">        [-3.9478,  2.5946],
</span></span><span class="line"><span class="cl">        [-0.2259,  0.1952]])
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">In [28]: keys
</span></span><span class="line"><span class="cl">Out[28]:
</span></span><span class="line"><span class="cl">tensor([[-1.9656,  0.2911],
</span></span><span class="line"><span class="cl">        [-2.9229,  3.0806],
</span></span><span class="line"><span class="cl">        [-0.6418,  0.5629]])
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">In [29]: values
</span></span><span class="line"><span class="cl">Out[29]:
</span></span><span class="line"><span class="cl">tensor([[-2.5787,  0.6903],
</span></span><span class="line"><span class="cl">        [-6.4049, -4.4522],
</span></span><span class="line"><span class="cl">        [-0.4950, -0.3706]])
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">In [30]: att_scores = querys @ keys.T
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">In [31]: att_scores
</span></span><span class="line"><span class="cl">Out[31]:
</span></span><span class="line"><span class="cl">tensor([[-0.9893, -1.4121, -0.3126],
</span></span><span class="line"><span class="cl">        [ 8.5152, 19.5319,  3.9940],
</span></span><span class="line"><span class="cl">        [ 0.5009,  1.2617,  0.2549]])
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">In [32]: att_scores_softmax = torch.nn.functional.softmax(att_scores, dim=-1)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">In [33]: att_scores_softmax
</span></span><span class="line"><span class="cl">Out[33]:
</span></span><span class="line"><span class="cl">tensor([[2.7604e-01, 1.8087e-01, 5.4310e-01],
</span></span><span class="line"><span class="cl">        [1.6426e-05, 9.9998e-01, 1.7865e-07],
</span></span><span class="line"><span class="cl">        [2.5498e-01, 5.4565e-01, 1.9937e-01]])
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">In [34]: outputs = att_scores_softmax @ values
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">In [35]: outputs
</span></span><span class="line"><span class="cl">Out[35]:
</span></span><span class="line"><span class="cl">tensor([[-2.1390, -0.8160],
</span></span><span class="line"><span class="cl">        [-6.4048, -4.4521],
</span></span><span class="line"><span class="cl">        [-4.2510, -2.3272]])
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">In [36]: outputs.shape
</span></span><span class="line"><span class="cl">Out[36]: torch.Size([3, 2])
</span></span></code></pre></td></tr></table>
</div>
</div></div><div class="post-footer" id="post-footer">
  <div class="post-info">
    <div class="post-info-line">
      <div class="post-info-mod">
        <span title=2023-05-07&#32;11:38:09>更新于 2023-05-07&nbsp;</span>
      </div></div>
    <div class="post-info-line">
      <div class="post-info-md"><span><a href="/selfattention%E5%92%8Cmultiheadattention%E6%B5%81%E7%A8%8B%E7%AE%80%E8%BF%B0/index.md" title="阅读原始文档" class="link-to-markdown">阅读原始文档</a></span></div>
      <div class="post-info-share">
        <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="https://blog-12x.pages.dev/selfattention%E5%92%8Cmultiheadattention%E6%B5%81%E7%A8%8B%E7%AE%80%E8%BF%B0/" data-title="SelfAttention和MultiHeadAttention流程简述" data-hashtags="draft"><i class="fa-brands fa-twitter fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://blog-12x.pages.dev/selfattention%E5%92%8Cmultiheadattention%E6%B5%81%E7%A8%8B%E7%AE%80%E8%BF%B0/" data-hashtag="draft"><i class="fa-brands fa-facebook-square fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 WhatsApp" data-sharer="whatsapp" data-url="https://blog-12x.pages.dev/selfattention%E5%92%8Cmultiheadattention%E6%B5%81%E7%A8%8B%E7%AE%80%E8%BF%B0/" data-title="SelfAttention和MultiHeadAttention流程简述" data-web><i class="fa-brands fa-whatsapp fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 Line" data-sharer="line" data-url="https://blog-12x.pages.dev/selfattention%E5%92%8Cmultiheadattention%E6%B5%81%E7%A8%8B%E7%AE%80%E8%BF%B0/" data-title="SelfAttention和MultiHeadAttention流程简述"><i data-svg-src="/lib/simple-icons/icons/line.min.svg" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://blog-12x.pages.dev/selfattention%E5%92%8Cmultiheadattention%E6%B5%81%E7%A8%8B%E7%AE%80%E8%BF%B0/" data-title="SelfAttention和MultiHeadAttention流程简述"><i class="fa-brands fa-weibo fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 Myspace" data-sharer="myspace" data-url="https://blog-12x.pages.dev/selfattention%E5%92%8Cmultiheadattention%E6%B5%81%E7%A8%8B%E7%AE%80%E8%BF%B0/" data-title="SelfAttention和MultiHeadAttention流程简述" data-description=""><i data-svg-src="/lib/simple-icons/icons/myspace.min.svg" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 Blogger" data-sharer="blogger" data-url="https://blog-12x.pages.dev/selfattention%E5%92%8Cmultiheadattention%E6%B5%81%E7%A8%8B%E7%AE%80%E8%BF%B0/" data-title="SelfAttention和MultiHeadAttention流程简述" data-description=""><i class="fa-brands fa-blogger fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 Evernote" data-sharer="evernote" data-url="https://blog-12x.pages.dev/selfattention%E5%92%8Cmultiheadattention%E6%B5%81%E7%A8%8B%E7%AE%80%E8%BF%B0/" data-title="SelfAttention和MultiHeadAttention流程简述"><i class="fa-brands fa-evernote fa-fw" aria-hidden="true"></i></a>
  </span>
      </div>
    </div>
  </div>

  <div class="post-info-more">
    <section class="post-tags"><i class="fa-solid fa-tags fa-fw me-1" aria-hidden="true"></i><a href='/tags/draft/' class="post-tag">draft</a></section>
    <section>
      <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
    </section>
  </div>

  <div class="post-nav"><a href="/%E4%BB%80%E4%B9%88%E6%98%AF%E4%BA%A4%E5%8F%89%E7%86%B5/" class="post-nav-item" rel="prev" title="什么是交叉熵"><i class="fa-solid fa-angle-left fa-fw" aria-hidden="true"></i>什么是交叉熵</a>
      <a href="/asyncio%E4%BD%BF%E7%94%A8/" class="post-nav-item" rel="next" title="Asyncio使用">Asyncio使用<i class="fa-solid fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
</article></main><footer class="footer">
    <div class="footer-container"><div class="footer-line powered">由 <a href="https://gohugo.io/" target="_blank" rel="external nofollow noopener noreferrer" title="Hugo 0.104.3">Hugo</a> 强力驱动 | 主题 - <a href="https://github.com/hugo-fixit/FixIt" target="_blank" rel="external" title="FixIt v0.2.18-RC"><img class="fixit-icon" src="/fixit.min.svg" alt="FixIt logo" />&nbsp;FixIt</a>
        </div><div class="footer-line copyright" itemscope itemtype="http://schema.org/CreativeWork"><i class="fa-regular fa-copyright fa-fw" aria-hidden="true"></i>
            <span itemprop="copyrightYear">2021 - 2023</span><span class="author" itemprop="copyrightHolder">
              <a href="/">技术小窝</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div><div class="footer-line statistics"></div><div class="footer-line visitor">
          <span id="busuanzi_container_site_uv" title='总访客数'><i class="fa-regular fa-user fa-fw" aria-hidden="true"></i>&nbsp;<span id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span></span><span id="busuanzi_container_site_pv" class="footer-divider" title='总访问量'><i class="fa-regular fa-eye fa-fw" aria-hidden="true"></i>&nbsp;<span id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span></span>
        </div></div>
  </footer></div><div class="widgets"><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role="button" aria-label="回到顶部"><i class="fa-solid fa-arrow-up fa-fw" aria-hidden="true"></i><span class="variant-numeric d-none">0%</span>
        </div></div><div id="mask"></div><noscript>
    <div class="noscript-warning">FixIt 主题在启用 JavaScript 的情况下效果最佳。</div>
  </noscript>
</div><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script src="/lib/autocomplete/autocomplete.min.js" defer></script><script src="/lib/lunr/lunr.min.js" defer></script><script src="/lib/lunr/lunr.stemmer.support.min.js" defer></script><script src="/lib/lunr/lunr.zh.min.js" defer></script><script src="/lib/sharer/sharer.min.js" async defer></script><script src="/lib/cookieconsent/cookieconsent.min.js" defer></script><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async defer></script><script>window.config={"code":{"copyTitle":"复制到剪贴板","editLockTitle":"锁定可编辑代码块","editUnLockTitle":"解锁可编辑代码块","editable":true,"maxShownLines":10},"comment":{"enable":false},"cookieconsent":{"content":{"dismiss":"同意","link":"了解更多","message":"本网站使用 Cookies 来改善您的浏览体验。"},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"enablePWA":true,"search":{"highlightTag":"em","lunrIndexURL":"/index.json","lunrLanguageCode":"zh","lunrSegmentitURL":"/lib/lunr/lunr.segmentit.js","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"lunr"}};</script><script src="/js/theme.min.js" defer></script></body>
</html>
